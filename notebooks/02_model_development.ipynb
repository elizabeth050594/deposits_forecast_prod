{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d451794-8872-4832-9495-5ab40d08de6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c90862ab-d940-4755-a023-53a03890d808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sktime.split import temporal_train_test_split\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error, mean_squared_error\n",
    "from sktime.forecasting.residual_booster import ResidualBoostingForecaster\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sktime.forecasting.compose import make_reduction\n",
    "from sktime.forecasting.ets import AutoETS\n",
    "from sktime.forecasting.model_evaluation import evaluate\n",
    "from sktime.forecasting.model_selection import ExpandingWindowSplitter\n",
    "from sktime.forecasting.compose import TransformedTargetForecaster\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import shap\n",
    "import json\n",
    "import re\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "from itertools import product\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "704b52cf-6ed0-46c3-b5c6-1b77961ea413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Development for Total Household Deposits\n",
    "\n",
    "This notebook documents the model development process for forecasting total household deposits. The forecasting pipeline combines an **AutoETS base forecaster** with a **residual boosting model** using **Gradient Boosting Regression**.\n",
    "\n",
    "The **AutoETS model** is a time series forecaster that automatically selects the best configuration of exponential smoothing (ETS) components â€” error, trend, and seasonality. It captures the primary linear, seasonal, and trend-driven structure of the time series. \n",
    "\n",
    "**Note: AutoETS is used instead of manually specifying ETS components as it automatically selects the best combination of Trend, Seasonality and Error using AICc. It provides a robust, consistent and interpretable baseline across time so that the residual booster can focus purely on what ETS can't explain and is relatively immune to overfitting*\n",
    "\n",
    "Given that household deposits exhibit strong linear, non-stationary and seasonal patterns, AutoETS serves as the ideal base model for capturing these dominant patterns. \n",
    "However, to enhance the forecast and capture **nonlinear relationships** or **exogenous effects** not explained by the base model, a **Gradient Boosting model is trained on the residuals** (errors) from the AutoETS forecast.\n",
    "\n",
    "This two-stage modeling approach is designed to combine the strengths of statistical forecasting and machine learning.  \n",
    "The development process consists of two key stages:  \n",
    "- **SHAP-Based Feature selection** using average SHAP importance across cross-validation folds, applied to the residual boosting model.  \n",
    "- **Hyperparameter tuning** of the Gradient Boosting regressor to optimize residual learning performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21b8cf86-4f0e-445a-83d1-09de60788001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_features(df, target_col, predictor_cols, lags=[1,3,4, 6,12], rolling_windows=[3,4, 6,12]):\n",
    "    \"\"\"\n",
    "    Creates lag-based and rolling statistical features for both the target and predictor columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame with 'date', target, and predictor columns\n",
    "    - target_col (str): Name of the target column\n",
    "    - predictor_cols (list): List of predictor column names\n",
    "    - lags (list): Lags to apply for lag, diff, and pct change features\n",
    "    - rolling_windows (list): Window sizes for rolling statistics (mean, std, min, max)\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with engineered features and datetime features (year, month, quarter)\n",
    "    \"\"\"\n",
    "    # Ensure date is in datetime format\n",
    "    df = df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date')\n",
    "    df.set_index('date', inplace=True)\n",
    "\n",
    "    new_features = {}\n",
    "\n",
    "    # For each predictor (and target), compute lags, rolling stats, etc.\n",
    "    for col in [target_col] + predictor_cols:\n",
    "        # Lag features\n",
    "        for lag in lags:\n",
    "            new_features[f'{col}_lag{lag}'] = df[col].shift(lag) # lag\n",
    "\n",
    "        # Differences and pct changes\n",
    "        for lag in lags:\n",
    "            new_features[f'{col}_diff{lag}'] = df[col].diff(lag) # difference\n",
    "            if (df[col] != 0).all():\n",
    "                new_features[f'{col}_roc{lag}'] = df[col].pct_change(lag) # rate of change\n",
    "\n",
    "        # Rolling stats\n",
    "        for win in rolling_windows:\n",
    "            new_features[f'{col}_ma{win}'] = df[col].rolling(win).mean()\n",
    "            new_features[f'{col}_std{win}'] = df[col].rolling(win).std()\n",
    "            new_features[f'{col}_min{win}'] = df[col].rolling(win).min()\n",
    "            new_features[f'{col}_max{win}'] = df[col].rolling(win).max()\n",
    "\n",
    "    # Combine original and new features        \n",
    "    df_new = pd.concat([df]+ [pd.DataFrame(new_features, index=df.index)], axis=1)\n",
    "\n",
    "    # Add datetime features\n",
    "    df_new['year'] = df_new.index.year\n",
    "    df_new['month'] = df_new.index.month\n",
    "    df_new['quarter'] = df_new.index.quarter\n",
    "\n",
    "    # Drop rows with missing values caused by shifting/rolling\n",
    "    df_new = df_new.dropna()\n",
    "    df_new.reset_index(inplace=True)\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a424c223-5ec4-43a6-9342-4a5bfaa520cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Read Data\n",
    "raw_df = pd.read_csv('../data/processed/combined_total_household_data_interpolate.csv')\n",
    "raw_df = raw_df.sort_values('date')\n",
    "\n",
    "# all significant features retained after the feature reduction phase\n",
    "significant_cols = ['card_consumables', 'card_credit','household_loans', 'google_home_loan_rate_search', 'unemployment_rate','employed_labour_force', 'labour_force_participation_rate','labour_cost_index', 'cpi', 'cpi_housing_household', 'cpi_qq', 'cpi_yy',\n",
    "'production_based_gdp', 'production_based_gdp_qq','production_based_gdp_yy']\n",
    "cpi_features = ['cpi', 'cpi_housing_household', 'cpi_qq', 'cpi_yy'] # cpi realted features, only one should be selected if any are significant at all\n",
    "gdp_features = ['production_based_gdp', 'production_based_gdp_qq','production_based_gdp_yy'] # gdp realted features, only one should be selected if any are significant at all\n",
    "\n",
    "raw_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "765dd2a5-f09a-4e42-8381-6886d5683725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_expanding_cv_splits(y, initial_window, step_length, fh):\n",
    "    \"\"\"\n",
    "    Plot expanding window CV with dates on the x-axis to experiment and visualize the CV train test splits.\n",
    "    \n",
    "    Parameters:\n",
    "        y (pd.Series): time series with PeriodIndex or DatetimeIndex\n",
    "        initial_window (int): initial training window size\n",
    "        step_length (int): step size between folds\n",
    "        fh (int): forecast horizon (e.g. 1, 2, etc.)\n",
    "    \"\"\"\n",
    "    if isinstance(y.index, pd.PeriodIndex):\n",
    "        x_dates = y.index.to_timestamp()\n",
    "    else:\n",
    "        x_dates = y.index\n",
    "\n",
    "    cv = ExpandingWindowSplitter(\n",
    "        initial_window=initial_window,\n",
    "        step_length=step_length,\n",
    "        fh=[fh]\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    n_splits = cv.get_n_splits(y)\n",
    "\n",
    "    test_dates = []\n",
    "    for i, (train_idx, test_idx) in enumerate(cv.split(y)):\n",
    "        plt.plot(x_dates[train_idx], [i] * len(train_idx), 'b.', label='Train' if i == 0 else \"\")\n",
    "        plt.plot(x_dates[test_idx], [i] * len(test_idx), 'r.', label='Test' if i == 0 else \"\")\n",
    "        test_dates.append(x_dates[test_idx][0])\n",
    "\n",
    "    formatted_test_dates = [d.strftime('%Y-%m') for d in test_dates]\n",
    "    print(\"All test dates:\", formatted_test_dates)\n",
    "    \n",
    "    plt.title(f\"Expanding Window CV â€” {n_splits} folds | Init={initial_window}, Step={step_length}, FH={fh}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Fold\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Target setup\n",
    "df_train, df_test = temporal_train_test_split(raw_df, test_size=6)\n",
    "df = df_train.set_index('date').asfreq('MS')\n",
    "df.index = df.index.to_period(\"M\")\n",
    "y = df['household_deposits']\n",
    "\n",
    "# Inital Parameters\n",
    "n_folds = 10\n",
    "horizon = 14\n",
    "N = len(y)\n",
    "step_length =  6\n",
    "\n",
    "# Back-calculate initial window so that last fold is recent\n",
    "initial_window = N - ((n_folds - 1) * step_length + horizon)\n",
    "\n",
    "print(f\"Initial window: {initial_window}, Step length: {step_length}\")\n",
    "\n",
    "# Plot\n",
    "plot_expanding_cv_splits(y, initial_window=initial_window, step_length=step_length, fh=horizon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53b70153-9a81-466c-ad8a-73736542d537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SHAP-Based Feature Selection Process\n",
    "\n",
    "To select the most predictive features for the residual component of the AutoETS + Gradient Boosting pipeline, a cross-validated SHAP-based feature selection procedure was implemented. The gradient booster for residual boosting is trained to learn and correct the residuals (forecast errors) produced by the AutoETS model.\n",
    "\n",
    "For each fold in an expanding window cross-validation:\n",
    "\n",
    "- A base AutoETS model is fit on a sub-window to forecast household deposits *h* months ahead.\n",
    "- Within each fold, a series of forecasts is made using expanding sub-windows, generating residuals between AutoETS predictions and actual values at horizon *h*.\n",
    "- These residuals are treated as the target for a Gradient Boosting Regressor (GBR), which is trained on exogenous features including lagged and rolling statistics.\n",
    "- SHAP values are computed for each trained GBR to assess feature contributions to residual prediction.\n",
    "- Absolute SHAP values are collected for all folds and averaged to produce stable, cross-validated feature importance rankings.\n",
    "\n",
    "At the end of the process, *k* residual models (one per fold) yield *k* sets of SHAP importances. These are averaged to determine the most consistently important features across time, and the top-ranked variables are selected for use in each horizon-specific forecast model. This method ensures that selected features are not only predictive but also temporally robust, capturing consistent patterns in forecast errors across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6829d9ae-de26-4e90-9144-a38ea6586411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# use shap\n",
    "def average_shap_feature_selector_cv(df,predictor_cols,lags=[1, 3, 4],rolling_windows=[3, 4],h=2,n_folds=5, step_length=10,sp=12,n_estimators=100, learning_rate=0.1,max_depth=3, initial_training_window = 80):\n",
    "    \"\"\"\n",
    "    Performs cross-validated SHAP-based feature importance estimation using residual boosting.\n",
    "\n",
    "    This function fits an AutoETS model to forecast `household_deposits`, computes residuals\n",
    "    at each horizon `h`, and fits a Gradient Boosting Regressor to model these residuals. \n",
    "    SHAP values are calculated on the boosting model to estimate feature importance across CV folds.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with a 'date' column, target 'household_deposits', and predictors.\n",
    "    predictor_cols : list\n",
    "        List of predictor variable names to engineer features from.\n",
    "    lags : list\n",
    "        List of lag values for feature engineering.\n",
    "    rolling_windows : list\n",
    "        Rolling window sizes for statistical feature creation.\n",
    "    h : int\n",
    "        Forecast horizon (e.g., 2 = forecast 2 months ahead).\n",
    "    n_folds : int\n",
    "        Number of cross-validation folds.\n",
    "    step_length : int\n",
    "        Step between CV fold start dates.\n",
    "    sp : int\n",
    "        Seasonal period for AutoETS (set to 12 for the deposits data as strong yearly seasonaility observed).\n",
    "    n_estimators : int\n",
    "        Number of trees for GradientBoostingRegressor.\n",
    "    learning_rate : float\n",
    "        Learning rate for the gradient boosting model.\n",
    "    max_depth : int\n",
    "        Maximum depth of trees in GBT model.\n",
    "    initial_training_window : int\n",
    "        Minimum number of points for inner expanding window (ETS model training).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    feature_importance : pd.DataFrame\n",
    "        DataFrame with 'feature', 'mean_abs_shap', and 'std_abs_shap', sorted by importance.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare Data\n",
    "    df = df[['date', 'household_deposits'] + predictor_cols].copy()\n",
    "    df = create_features(df, 'household_deposits', predictor_cols, lags=lags, rolling_windows=rolling_windows)\n",
    "    df = df.set_index('date').asfreq('MS')\n",
    "    df.index = df.index.to_period(\"M\")\n",
    "    X = df.drop(columns='household_deposits')\n",
    "    y = df['household_deposits']\n",
    "    N = len(X)\n",
    "\n",
    "    # Define initial window for outer cross-validation\n",
    "    initial_window = N - ((n_folds - 1) * step_length + h)\n",
    "\n",
    "    splitter = ExpandingWindowSplitter(\n",
    "        initial_window=initial_window,\n",
    "        step_length=step_length,\n",
    "        fh=[h]\n",
    "    )\n",
    "\n",
    "    shap_list = []\n",
    "    feature_names = X.columns\n",
    "\n",
    "    # Cross-validated Residual Modeling\n",
    "    for fold_i, (train_idx, test_idx) in enumerate(splitter.split(y)):\n",
    "        print(f\"Fold {fold_i+1}/{n_folds}\")\n",
    "\n",
    "        residuals_list = []\n",
    "        features_list = []\n",
    "        print(f'start: {initial_training_window} --> end: {train_idx[-h]}')\n",
    "        # Inner expanding window loop to simulate walk-forward residual estimation\n",
    "        for j in range(initial_training_window, train_idx[-h] + 1):  # expand from initial window to fold train end\n",
    "            y_train_sub = y.iloc[:j]\n",
    "            X_train_sub = X.iloc[:j]\n",
    "\n",
    "            # Train ETS on sub-window\n",
    "            ets = AutoETS(auto=True, sp=sp, n_jobs=-1)\n",
    "            ets.fit(y_train_sub)\n",
    "\n",
    "            # Predict h steps ahead\n",
    "            y_pred = ets.predict(fh=[h])\n",
    "\n",
    "            # Actual value at forecast date\n",
    "            forecast_timestamp = y_pred.index[0]\n",
    "            actual_val = y.loc[forecast_timestamp]  \n",
    "\n",
    "            # Residual at horizon h\n",
    "            residual = actual_val - y_pred.iloc[0]\n",
    "\n",
    "            # Feature vector aligned with actual_val date\n",
    "            X_feat = X.loc[forecast_timestamp]\n",
    "\n",
    "            # Collect the residuals at every time point\n",
    "            residuals_list.append(residual)\n",
    "            features_list.append(X_feat)\n",
    "\n",
    "\n",
    "        # Train gradient boosting on residuals\n",
    "        residuals_fold = pd.Series(residuals_list)\n",
    "        X_resid_fold = pd.DataFrame(features_list)\n",
    "\n",
    "        reg = GradientBoostingRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            random_state=42\n",
    "        )\n",
    "        reg.fit(X_resid_fold, residuals_fold)\n",
    "\n",
    "        # Compute SHAP values\n",
    "        explainer = shap.TreeExplainer(reg)\n",
    "        shap_vals = explainer.shap_values(X_resid_fold)\n",
    "\n",
    "        # Store abs SHAP values for this fold\n",
    "        shap_list.append(np.abs(shap_vals))\n",
    "\n",
    "    # Aggregate SHAP over folds\n",
    "    shap_matrix = np.vstack(shap_list)\n",
    "    mean_abs_shap = shap_matrix.mean(axis=0)\n",
    "    std_abs_shap = shap_matrix.std(axis=0)\n",
    "\n",
    "    # Construct feature importance DataFrame\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'mean_abs_shap': mean_abs_shap,\n",
    "        'std_abs_shap': std_abs_shap\n",
    "    }).sort_values(by='mean_abs_shap', ascending=False)\n",
    "\n",
    "    print(\"Average SHAP feature importance across folds:\")\n",
    "    print(feature_importance.head(20))\n",
    "\n",
    "    return feature_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3ea19d6-9e12-4cb0-959f-c8c41f6e256f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SHAP-Based Feature Selection\n",
    "\n",
    "# Initialise inputs and parameters\n",
    "significant_cols = [\n",
    "    'card_consumables', 'card_credit', 'household_loans', 'google_home_loan_rate_search',\n",
    "    'unemployment_rate', 'employed_labour_force', 'labour_force_participation_rate',\n",
    "    'labour_cost_index', 'cpi', 'cpi_housing_household', 'cpi_qq', 'cpi_yy',\n",
    "    'production_based_gdp', 'production_based_gdp_qq', 'production_based_gdp_yy'\n",
    "]\n",
    "\n",
    "cpi_features = ['cpi', 'cpi_housing_household', 'cpi_qq', 'cpi_yy']\n",
    "gdp_features = ['production_based_gdp', 'production_based_gdp_qq', 'production_based_gdp_yy']\n",
    "horizons = [2, 5, 8, 14]\n",
    "\n",
    "# Hold out the last 6 months (small test set to maximize training for SHAP\n",
    "df_train, df_test = temporal_train_test_split(raw_df, test_size=6) \n",
    "shap_dict = {}\n",
    "\n",
    "# Run SHAP feature selection for each horizon\n",
    "for h in horizons:\n",
    "    print(f\"\\n=== Running SHAP selection for Horizon {h} ===\")\n",
    "    \n",
    "    shap_importance = average_shap_feature_selector_cv(\n",
    "        df=df_train,\n",
    "        predictor_cols=significant_cols,\n",
    "        lags=[1,2,3,4,5,6,7,8,9,10,11,12],              # Lags 1 to 12 - include all lags\n",
    "        rolling_windows=[2,3,4,5,6,7,8,9,10,11,12],     # Rolling windows 2 to 12 - include all rolling features\n",
    "        h=h,\n",
    "        n_folds=10,\n",
    "        step_length=6,\n",
    "        sp=12,\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4\n",
    "    )\n",
    "    \n",
    "    # Save and summarize\n",
    "    shap_df = pd.DataFrame(shap_importance)\n",
    "    shap_df.to_csv(f'tuning_results/shap_feature_selection_results_h{h}.csv')\n",
    "    mean_shap = shap_df['mean_abs_shap'].mean()\n",
    "    print(f\"Mean SHAP for horizon {h}: {mean_shap:.6f}\")\n",
    "\n",
    "    shap_df = shap_df[shap_df['mean_abs_shap'] > 0]\n",
    "    shap_dict[h] = shap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab378c95-38d2-44c5-89e6-52a94b1e1503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Utilities for SHAP visualisations\n",
    "def base_feat_name(feat):\n",
    "    \"\"\"Extract base feature name by removing lag/rolling/stat suffixes.\"\"\"\n",
    "    return re.sub(r'_(lag|ma|std|min|max|diff|roc)[0-9]+$', '', feat)\n",
    "\n",
    "def make_rank_heatmap(shap_dict, top_n=20):\n",
    "    \"\"\"Create heatmap showing feature ranks across horizons (based on mean SHAP).\"\"\"\n",
    "    all_feats = set()\n",
    "    for df in shap_dict.values():\n",
    "        df['base_feature'] = df['feature'].apply(base_feat_name)\n",
    "        all_feats.update(df['base_feature'].unique())\n",
    "\n",
    "    rank_data = pd.DataFrame(index=sorted(all_feats), columns=shap_dict.keys())\n",
    "\n",
    "    for h, df in shap_dict.items():\n",
    "        df['base_feature'] = df['feature'].apply(base_feat_name)\n",
    "        grouped = df.groupby('base_feature')['mean_abs_shap'].mean()\n",
    "        ranked = grouped.rank(ascending=False, method='min')\n",
    "        rank_data.loc[ranked.index, h] = ranked.astype('Int64')\n",
    "\n",
    "    # Get top N features across all horizons\n",
    "    min_ranks = pd.to_numeric(rank_data.min(axis=1), errors='coerce')\n",
    "    top_feats = min_ranks.nsmallest(top_n).index\n",
    "    rank_data = rank_data.loc[top_feats]\n",
    "\n",
    "    plt.figure(figsize=(10, max(6, len(top_feats) * 0.3)))\n",
    "    sns.heatmap(rank_data.astype(float), annot=True, cmap=\"YlOrRd_r\", cbar_kws={'label': 'Rank (1 = most important)'})\n",
    "    plt.xlabel(\"Forecast Horizon (months)\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.title(f\"SHAP Feature Rank Across Horizons (Top {top_n})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_shap_summary(shap_df, horizon, top_k=10):\n",
    "    \"\"\"Plot SHAP importance for top raw and base features.\"\"\"\n",
    "\n",
    "    shap_df = shap_df.copy()\n",
    "    shap_df['base_feature'] = shap_df['feature'].apply(base_feat_name)\n",
    "\n",
    "    # Top raw features\n",
    "    top_feats = shap_df.sort_values('mean_abs_shap', ascending=False).head(top_k)\n",
    "\n",
    "    # Grouped bar chart: mean and std side by side\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    x = range(len(top_feats))\n",
    "    bar_width = 0.4\n",
    "\n",
    "    ax.bar([i - bar_width/2 for i in x], top_feats['mean_abs_shap'], width=bar_width, label='Mean |SHAP|', color='skyblue')\n",
    "    ax.bar([i + bar_width/2 for i in x], top_feats['std_abs_shap'], width=bar_width, label='Std |SHAP|', color='salmon')\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(top_feats['feature'], rotation=45, ha='right')\n",
    "    ax.set_title(f\"H{horizon} - Raw Feature SHAP (Top {top_k})\")\n",
    "    ax.set_ylabel(\"|SHAP value|\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Aggregated by base feature (sum & mean)\n",
    "    base_summary = shap_df.groupby('base_feature').agg(\n",
    "        mean_shap=('mean_abs_shap', 'mean'),\n",
    "        sum_shap=('mean_abs_shap', 'sum')\n",
    "    ).sort_values('sum_shap', ascending=False)\n",
    "\n",
    "    top_base = base_summary.head(top_k)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "    axs[0].barh(top_base.index[::-1], top_base['sum_shap'][::-1], color='steelblue')\n",
    "    axs[0].set_title(f\"H{horizon} - Base Features by Sum |SHAP|\")\n",
    "    axs[0].set_xlabel(\"Sum |SHAP value|\")\n",
    "\n",
    "    axs[1].barh(top_base.index[::-1], top_base['mean_shap'][::-1], color='mediumseagreen')\n",
    "    axs[1].set_title(f\"H{horizon} - Base Features by Mean |SHAP|\")\n",
    "    axs[1].set_xlabel(\"Mean |SHAP value|\")\n",
    "\n",
    "    plt.suptitle(f\"H{horizon} - Aggregated Base Feature Importance (Top {top_k})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize the Shap results\n",
    "shap_dict = {}\n",
    "horizons = [2, 5, 8, 14]  # Or load dynamically\n",
    "\n",
    "for h in horizons:\n",
    "    print(f\"\\n=== Horizon {h} ===\")\n",
    "    shap_df = pd.read_csv(f'tuning_results/shap_feature_selection_results_h{h}.csv')\n",
    "    shap_dict[h] = shap_df\n",
    "    plot_shap_summary(shap_df, horizon=h, top_k=10)\n",
    "\n",
    "# Final heatmap across all horizons\n",
    "make_rank_heatmap(shap_dict, top_n=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6b395a3-be8f-4071-b8af-21481181ec49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Feature Selection for each Horizon**\n",
    "\n",
    "From the SHAP summary visualizations above, the following features were selected for each model:\n",
    "\n",
    "- **2-month horizon**: `'household_deposits_roc2'`, `'household_deposits_diff2'`, `'household_deposits_std3'`\n",
    "- **5-month horizon**: `'household_deposits_roc5'`, `'household_deposits_diff5'`\n",
    "- **8-month horizon**: `'household_deposits_roc8'`, `'household_deposits_diff8'`\n",
    "- **14-month horizon**: `'household_deposits_diff12'`, `'household_deposits_diff11'`, `'household_loans_roc4'`, `'household_loans_roc10'`\n",
    "\n",
    "Feature selection was guided by the SHAP analysis â€” specifically, **features with consistently high average SHAP values across cross-validation folds and relatively low standard deviation** were preferred. This approach favours stability and avoids selecting features that may appear important in only a few folds due to noise.\n",
    "\n",
    "For example, although `cpi_housing_household_diff12` had a high mean SHAP value for the 14-month horizon, its standard deviation was nearly twice as large, indicating unstable importance across folds. As such, it was excluded from the final feature set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7eca169-c989-4a2f-a872-4a6e28cde70e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "To optimize the performance of the residual Gradient Boosting model in the forecasting pipeline, a hyperparameter tuning process is performed using **Optuna**, with trial logging via **MLflow**. For each forecast horizon, the model is trained using **expanding window cross-validation**, where the final 12 months of the training set are held out for final testing evaluation. The objective function minimizes the **Root Mean Squared Error (RMSE)** of forecasts at the specified horizon.\n",
    "\n",
    "The tuning process searches over key hyperparameters of the Gradient Boosting model and its rolling window configuration, including:\n",
    "- Number of trees (`n_estimators`)\n",
    "- Learning rate (`learning_rate`)\n",
    "- Maximum tree depth (`max_depth`)\n",
    "- Rolling window length (`window_length`)\n",
    "- Subsampling rate (`subsample`)\n",
    "- Minimum samples per leaf (`min_samples_leaf`)\n",
    "\n",
    "Early stopping is applied to prevent excessive trials once performance stabilizes. After tuning, the best configuration is used to re-train the final model on the full training data, and performance metrics and forecasts are logged for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b37e5cc7-e42e-4bdd-941e-cc9c76caf07b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "def forecast_pipeline(X, y, horizon, window_length = 120, sp = 12, n_estimators=200, learning_rate=0.05, max_depth=4, step_length = 1, initial_window = 120, subsample = 1.0, min_samples_leaf = 1):\n",
    "    \"\"\"\n",
    "    Runs the full forecasting pipeline using AutoETS + Gradient Boosting.\n",
    "\n",
    "    Parameters:\n",
    "        X (pd.DataFrame): Feature matrix.\n",
    "        y (pd.Series): Target time series.\n",
    "        horizon (int): Forecast horizon (e.g., 2 months ahead).\n",
    "        window_length (int): Number of past points used in GBT residual correction.\n",
    "        sp (int): Seasonal period (set to 12 for the deposits data as strong yearly seasonaility observed).\n",
    "        n_estimators (int): Number of trees for Gradient Boosting.\n",
    "        learning_rate (float): Learning rate for Gradient Boosting.\n",
    "        max_depth (int): Max depth of each tree.\n",
    "        step_length (int): Step size between CV folds.\n",
    "        initial_window (int): Initial training window for expanding CV.\n",
    "        subsample (float): Fraction of samples to use per tree (for GB).\n",
    "        min_samples_leaf (int): Minimum samples per leaf in GB trees.\n",
    "\n",
    "    Returns:\n",
    "        results (pd.DataFrame): Forecast vs actual results with timestamps.\n",
    "        rmse (float): Root Mean Squared Error of predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Base ETS model\n",
    "    base_forecaster = AutoETS(auto=True,sp = sp, n_jobs = -1)\n",
    "\n",
    "    # Residual model (GBR) \n",
    "    regressor = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth, subsample = subsample, min_samples_leaf = min_samples_leaf, random_state = 42)\n",
    "    residual_forecaster = make_reduction(regressor, window_length=window_length, strategy=\"direct\")\n",
    "    residual_forecaster = TransformedTargetForecaster([\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "        (\"regressor\", residual_forecaster)\n",
    "    ])\n",
    "\n",
    "    # Combined residual boosting forecast model\n",
    "    forecaster = ResidualBoostingForecaster(base_forecaster, residual_forecaster)\n",
    "\n",
    "    # Expanding window cross-validation\n",
    "    splitter = ExpandingWindowSplitter(\n",
    "        initial_window=initial_window,\n",
    "        step_length=step_length,\n",
    "        fh=[horizon]\n",
    "    )\n",
    "\n",
    "    # Run evaluation\n",
    "    cv_results = evaluate(\n",
    "        forecaster=forecaster,\n",
    "        y=y,\n",
    "        X = X,\n",
    "        cv=splitter,\n",
    "        strategy=\"refit\",\n",
    "        return_data=True,   \n",
    "    )\n",
    "\n",
    "    # Collect predictions and true values\n",
    "    results = pd.DataFrame(\n",
    "        {'date':[s.index[0].to_timestamp() for s in cv_results[\"y_pred\"]],\n",
    "        'y_true' :[s.values[0] for s in cv_results[\"y_test\"]],\n",
    "        'y_pred' :[s.values[0] for s in cv_results[\"y_pred\"]]})\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(results.y_true, results.y_pred))\n",
    "    return results, rmse\n",
    "\n",
    "def run_forecast_with_lags_and_windows(df, h=2, n_folds=12, step_length = 10, window_length=120, sp=12, n_estimators=100, learning_rate=0.1, max_depth=3, subsample = 1, min_samples_leaf = 1):\n",
    "\n",
    "    \"\"\"Wrapper for running forecast pipeline with CV-friendly expanding window setup. See forecast_pipeline for parameter details.\"\"\"\n",
    "\n",
    "    X = df.drop(columns='household_deposits')\n",
    "    y = df['household_deposits']\n",
    "\n",
    "\n",
    "    # Compute initial window to get approximately n_folds with given step_length\n",
    "    N = len(X)\n",
    "    initial_window = N - ((n_folds - 1) * step_length + h)  # Back-calculate initial window so that last fold is recent\n",
    "\n",
    "    results, rmse = forecast_pipeline(\n",
    "            X, y,\n",
    "            horizon=h,\n",
    "            window_length=window_length,\n",
    "            sp=sp,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample = subsample, \n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            step_length = step_length,\n",
    "            initial_window = initial_window\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "            \"rmse\": rmse,\n",
    "            \"results\": results\n",
    "        }\n",
    "    \n",
    "# Optuna Tuning definition\n",
    "def early_stopping_callback(patience=30):\n",
    "    \"\"\"Returns a callback that stops the study if no improvement is seen in 'patience' trials.\"\"\"\n",
    "    def callback(study, trial):\n",
    "        if study.best_trial.number + patience <= trial.number:\n",
    "            print(f\"\\nEarly stopping triggered: No improvement in the last {patience} trials.\")\n",
    "            study.stop()\n",
    "    return callback\n",
    "\n",
    "def objective(trial, df, h=2, n_folds=10, step_length=6):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to minimize RMSE.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial): Optuna trial object.\n",
    "        df (pd.DataFrame): Feature matrix + target.\n",
    "        h (int): Forecast horizon.\n",
    "        n_folds (int): Number of CV folds.\n",
    "        step_length (int): Step size for CV.\n",
    "\n",
    "    Returns:\n",
    "        float: RMSE of the trial configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameter search space\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 300)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.1)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2,5)\n",
    "    window_length = trial.suggest_int(\"window_length\", 90, 200)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.7, 1.0)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 5)\n",
    "\n",
    "\n",
    "    try:\n",
    "        result = run_forecast_with_lags_and_windows(\n",
    "            df=df,\n",
    "            h=h,\n",
    "            n_folds=n_folds,\n",
    "            step_length=step_length,\n",
    "            window_length=window_length,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample = subsample, \n",
    "            min_samples_leaf = min_samples_leaf\n",
    "        )\n",
    "    except Exception as e:\n",
    "        trial.set_user_attr(\"error\", str(e))\n",
    "        return float(\"inf\")\n",
    "\n",
    "    rmse = result[\"rmse\"]\n",
    "\n",
    "    # Log trial parameters and metric manually to MLflow\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_param(\"features\", \",\".join(df.columns))\n",
    "        mlflow.log_params({\n",
    "            \"sp\": 12,\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"window_length\": window_length,\n",
    "            \"subsample\":subsample,\n",
    "            \"min_samples_leaf\":min_samples_leaf, \n",
    "            \"horizon\": h\n",
    "        })\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "def tune_with_optuna_mlflow(df,h=2,n_trials=50,n_folds=10,step_length=6):\n",
    "    \"\"\"\n",
    "    Runs hyperparameter tuning using Optuna and logs results to MLflow.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset with features and target.\n",
    "        h (int): Forecast horizon.\n",
    "        n_trials (int): Max number of Optuna trials.\n",
    "        n_folds (int): Number of CV folds.\n",
    "        step_length (int): Step size for CV.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[optuna.Study, dict]: The Optuna study and final results dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    study = optuna.create_study(study_name=f\"forecast_study_h{h}\", direction=\"minimize\", sampler = optuna.samplers.TPESampler(seed=42))\n",
    "\n",
    "\n",
    "    # Optimize with early stopping\n",
    "    study.optimize(\n",
    "        lambda trial: objective(\n",
    "            trial,\n",
    "            df=df,\n",
    "            h=h,\n",
    "            n_folds=n_folds,\n",
    "            step_length=step_length,\n",
    "        ),\n",
    "        n_trials=n_trials,\n",
    "        callbacks=[early_stopping_callback()],\n",
    "        show_progress_bar=False,\n",
    "    )\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    print(study.best_trial.params)\n",
    "    print(\"Best RMSE:\", study.best_value)\n",
    "\n",
    "    # After optimization, re-run final model with best params\n",
    "    best_params = study.best_trial.params\n",
    "\n",
    "    best_result = run_forecast_with_lags_and_windows(\n",
    "        df=df,\n",
    "        h=h,\n",
    "        n_folds=n_folds,\n",
    "        step_length=step_length,\n",
    "        window_length=best_params[\"window_length\"],\n",
    "        sp=12,\n",
    "        n_estimators=best_params[\"n_estimators\"],\n",
    "        learning_rate=best_params[\"learning_rate\"],\n",
    "        max_depth=best_params[\"max_depth\"],\n",
    "        subsample=best_params[\"subsample\"],\n",
    "        min_samples_leaf=best_params[\"min_samples_leaf\"]\n",
    "    )\n",
    "\n",
    "    # Log the final best model and results\n",
    "    with mlflow.start_run(run_name=f\"shap_final_model_h{h}\", nested=True):\n",
    "        mlflow.log_params(best_params)\n",
    "        mlflow.log_metric(\"rmse_final\", best_result[\"rmse\"])\n",
    "\n",
    "        results_df = best_result[\"results\"]\n",
    "        csv_path = f\"tuning_results/Shap_optuna_final_results_h{h}.csv\"\n",
    "        results_df.to_csv(csv_path, index=False)\n",
    "        mlflow.log_artifact(csv_path)\n",
    "\n",
    "        df_trials = study.trials_dataframe()\n",
    "        csv_trials_path = f\"tuning_results/Shap_optuna_trial_history_h{h}.csv\"\n",
    "        df_trials.to_csv(csv_trials_path, index=False)\n",
    "        mlflow.log_artifact(csv_trials_path)\n",
    "\n",
    "\n",
    "    return study, best_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b2030cc-377c-473c-837e-c576b3fa95f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**A note on AutoETS parameters**:  \n",
    "AutoETS automatically selects the optimal combination of error, trend, and seasonality components based on the corrected Akaike Information Criterion (AICc), making it highly adaptive to changes in the underlying temporal structure of the data. Unlike manually specifying a fixed ETS configuration â€” which may be more appropriate for stable and predictable time series â€” AutoETS dynamically re-evaluates and adjusts its components (e.g., from additive to damped trends, or seasonal to non-seasonal) as new data becomes available. This adaptability is particularly valuable in real-world forecasting, where patterns can evolve over time due to economic shocks, policy interventions, or structural changes. As a result, AutoETS offers a more flexible and scalable solution for time series modeling in dynamic environments.\n",
    "\n",
    "A key assumption in this pipeline is that AutoETS will always return the best-fitting ETS structure given the available data at each point in time â€” effectively capturing the components that are explainable by traditional time series decomposition. This allows the **residual Gradient Boosting model** to focus on learning any remaining nonlinear patterns or relationships driven by exogenous variables that AutoETS cannot account for.\n",
    "\n",
    "Cross-validation further reinforces this setup: with 10 folds and a 6-month step length, the model evaluates a range of data windows â€” each potentially resulting in different AutoETS configurations. This variability exposes the boosting model to a diverse set of residual patterns, strengthening its ability to generalize and complement the base model.\n",
    "\n",
    "The only manually specified parameter for AutoETS is the **seasonal periodicity (`sp`)**, which defines the recurring seasonal cycle. For monthly data, `sp=12` captures annual seasonality, while `sp=4` would be appropriate for quarterly data.  \n",
    "Exploratory analysis (see below) confirms a strong and stable annual pattern in household deposits, justifying the choice of `sp=12`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "989ad155-38d0-48cf-8973-059f2e370042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Load and clean time series\n",
    "y = raw_df['household_deposits']\n",
    "\n",
    "# Decompose with different seasonal periods\n",
    "periods = [1, 4, 6, 12]\n",
    "decompositions = [seasonal_decompose(y, period=p) for p in periods]\n",
    "\n",
    "# Plot seasonal components in 2x2 grid\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8), sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, p in enumerate(periods):\n",
    "    decompositions[i].seasonal.plot(ax=axes[i])\n",
    "    axes[i].set_title(f\"Seasonal Component (sp={p})\")\n",
    "    axes[i].set_ylabel(\"Seasonality\")\n",
    "\n",
    "plt.suptitle(\"Seasonal Decomposition of Household Deposits\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "058ba569-7473-4434-990c-dd4a6f57bf11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Observe clear seasonality trend for sp = 12 which is also expected for monthly data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3ac7609-24ce-41e3-989f-6b4ade55125f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Hyperparameter Tuning for Residual Gradient Booster**\n",
    "\n",
    "The code below performs hyperparameter tuning for the residual gradient booster as described in the earlier section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c320d9d2-1998-463a-95bb-0b085b2d61cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "# Initial setup based on SHAP importance results\n",
    "best_features = {\n",
    "    2:['household_deposits_roc2','household_deposits_diff2','household_deposits_std3'],\n",
    "    5:['household_deposits_roc5', 'household_deposits_diff5'],\n",
    "    8:['household_deposits_roc8','household_deposits_diff8'],\n",
    "    14:['household_deposits_diff12', 'household_deposits_diff11', 'household_loans_roc4', 'household_loans_roc10']}\n",
    "\n",
    "best_lags = {\n",
    "    2:[2],\n",
    "    5:[5],\n",
    "    8:[8],\n",
    "    14:[4,10,11,12]\n",
    "}\n",
    "\n",
    "best_rolling_windows = {\n",
    "    2:[3],\n",
    "    5:[],\n",
    "    8:[],\n",
    "    14:[]\n",
    "}\n",
    "\n",
    "horizons = [2,5,8,14]\n",
    "df_train, df_test = temporal_train_test_split(raw_df, test_size=12)\n",
    "\n",
    "# Run tuning\n",
    "for h in horizons:\n",
    "    df = df_train[['date', 'household_deposits','household_loans']].copy()\n",
    "    predictor_cols = [x for x in df.columns if x not in ['date', 'household_deposits']]\n",
    "    # Feature engineering with variable lags/windows\n",
    "    df_feats = create_features(df, 'household_deposits', predictor_cols, lags=best_lags[h], rolling_windows=best_rolling_windows[h])\n",
    "    df_feats = df_feats.set_index('date')\n",
    "    df_feats = df_feats.asfreq('MS')\n",
    "    df_feats.index = df_feats.index.to_period(\"M\")\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"Parameter_Tuning_h{h}\"):\n",
    "        study, best_result = tune_with_optuna_mlflow(\n",
    "            df = df_feats[['household_deposits']+best_features[h]],\n",
    "            h = h,\n",
    "            n_trials = 100,       \n",
    "            n_folds = 10,\n",
    "            step_length = 6\n",
    "        )\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34a40117-f783-4fbd-9d3e-4cedd217a61c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Save the parameters \n",
    "# Combine all the results from the feature selection and hyperparameter tuning into a final parameter dictionary for each of the horizon forecast models\n",
    "\n",
    "# Final parameter dictionary\n",
    "param_dict = {'model_forecast_h14': {'learning_rate':0.09556428757689246, 'max_depth':4, 'min_samples_leaf':1, 'n_estimators':175, 'sp':12, 'subsample':0.7468055921327309, 'window_length':156, 'best_features':['household_deposits_diff12', 'household_deposits_diff11', 'household_loans_roc4', 'household_loans_roc10'], 'predictors':['household_loans'],'best_lags':[4,10,11,12], 'best_rolling_windows':[],'horizon':14},\n",
    " 'model_forecast_h8': {'learning_rate':0.0323887571145074, 'max_depth':4, 'min_samples_leaf':1, 'n_estimators':277, 'sp':12, 'subsample':0.7629576088119175, 'window_length':150, 'best_features':['household_deposits_roc8','household_deposits_diff8'], 'predictors':[], 'best_lags':[8], 'best_rolling_windows':[],'horizon':8},\n",
    " 'model_forecast_h5': {'learning_rate':0.08240638205278353, 'max_depth':3, 'min_samples_leaf':1, 'n_estimators':119, 'sp':12, 'subsample':0.9521611575184935, 'window_length':129, 'best_features':['household_deposits_roc5', 'household_deposits_diff5'], 'predictors':[], 'best_lags':[5], 'best_rolling_windows':[],'horizon':5},\n",
    " 'model_forecast_h2': {'learning_rate':0.07344290290006429, 'max_depth':4, 'min_samples_leaf':3, 'n_estimators':200, 'sp':12, 'subsample':0.7643393094474292, 'window_length':160, 'best_features':['household_deposits_roc2','household_deposits_diff2','household_deposits_std3'], 'predictors':[], 'best_lags':[2], 'best_rolling_windows':[3],'horizon':2},\n",
    "  'model_forecast_h1': {'learning_rate':0.07344290290006429, 'max_depth':4, 'min_samples_leaf':3, 'n_estimators':200, 'sp':12, 'subsample':0.7643393094474292, 'window_length':160, 'best_features':['household_deposits_roc2','household_deposits_diff2','household_deposits_std3'], 'predictors':[], 'best_lags':[2], 'best_rolling_windows':[3],'horizon':1}         \n",
    "}\n",
    "\n",
    "with open(\"model_configs.json\", \"w\") as f:\n",
    "    json.dump(param_dict, f, indent=4)\n",
    "\n",
    "# Save separately for production\n",
    "# Create a directory to store config files if it doesn't exist\n",
    "config_dir = Path(\"../configs\")\n",
    "config_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save each config to a separate file\n",
    "for model_name, params in param_dict.items():\n",
    "    config_path = config_dir / f\"{model_name}.json\"\n",
    "    params['model_name'] = model_name\n",
    "    params['target'] = 'household_deposits'\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(params, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d83c67db-221f-4a58-9c27-2d28e6e15074",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_model_development",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
