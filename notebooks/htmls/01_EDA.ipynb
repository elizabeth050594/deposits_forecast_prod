{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "281452bf-3d5e-4d56-89d4-c2665c0a1adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323c67aa-3ac8-48ac-9437-3d1708bf3901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.offline import plot\n",
    "from plotly.subplots import make_subplots\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import plotly.graph_objects as go\n",
    "from functools import reduce \n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb45d666-8bb0-468c-8480-35e5a4dcb59c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exploratory Data Analysis for Total Household Deposits\n",
    "\n",
    "This notebook documents the exploratory data analysis (EDA) conducted on the original set of 49 variables. Following the initial analysis and subsequent feature reduction, the majority of these variables were excluded from further processing. Only a small subset of relevant features was retained and carried forward into the feature selection and model development stages (see `00_data_processing` for exact list of remaining features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8148faf8-f4f9-4d1f-8245-7f963ee0f620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read data (note this uses legacy data which has the original 49 variables initially used for exploration)\n",
    "filepath = '../data/processed/legacy_combined_df_for_eda.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "df = df[(df.date >= '2004-01') & (df.date<'2025-04')]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdcd85e1-ea20-4f7e-8adf-416b463c24fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Granger Causality and Lagged Correlation Analysis\n",
    "\n",
    "This section investigates the temporal relationships between the target variable and the predictor features using two complementary approaches: the Granger causality test and lagged correlation analysis. \n",
    "\n",
    "Since the raw time series data can include overlapping trends, seasonality, and residual noise, STL decomposition was first applied to separate each series into its **trend**, **seasonal**, and **residual** components. Granger causality tests were then performed on each component individually to identify features that have statistically significant predictive power for the target variable across various time lags (up to 12 months). Features were considered causally related if any lag produced a p-value â‰¤ 0.05.\n",
    "\n",
    "In parallel, lagged Pearson correlations were computed to identify features that are strongly correlated with the target variable at specific lags. This provides additional insight into the temporal alignment of feature-target relationships, even if those relationships are not strictly causal.\n",
    "\n",
    "By analyzing each STL component separately, this approach helps to uncover both predictive and contemporaneous relationships that may not be visible in the raw data due to noise or confounding temporal patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e291318-ee7a-4760-a96f-f180ceef5feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def decompose_stl(df, date_col, target_col, predictor_cols, period=12):\n",
    "    df = df.sort_values(date_col).copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df.set_index(date_col, inplace=True)\n",
    "\n",
    "    all_cols = [target_col] + predictor_cols\n",
    "\n",
    "    trend_data = {}\n",
    "    seasonal_data = {}\n",
    "    residual_data = {}\n",
    "\n",
    "    for col in all_cols:\n",
    "        series = pd.to_numeric(df[col], errors='coerce')\n",
    "        stl = STL(series, period=period).fit()\n",
    "\n",
    "        trend_data[col] = stl.trend\n",
    "        seasonal_data[col] = stl.seasonal\n",
    "        residual_data[col] = stl.resid\n",
    "\n",
    "    # Create three separate DataFrames each for the STL decompositions\n",
    "    trend_df = pd.DataFrame(trend_data, index=df.index)\n",
    "    seasonal_df = pd.DataFrame(seasonal_data, index=df.index)\n",
    "    residual_df = pd.DataFrame(residual_data, index=df.index)\n",
    "\n",
    "    return trend_df, seasonal_df, residual_df\n",
    "\n",
    "def compute_df_correlation(df, date_col = 'date'):\n",
    "    return df.corr()\n",
    "\n",
    "def plot_target_correlation(df, target_col, type = 'trend'):\n",
    "    df = df[[target_col]].copy()\n",
    "    df[f'abs_{type}_correlation'] = df[target_col].abs()\n",
    "    df.sort_values(f'abs_{type}_correlation', ascending=False, inplace=True)\n",
    "    # plot heatmap\n",
    "    plt.figure(figsize=(10, 15)) \n",
    "    sns.heatmap(df[[f'abs_{type}_correlation']], annot=True, cmap='coolwarm', vmin = 0, vmax = 1)\n",
    "    plt.title(f'Correlation with Household Deposit ({type})')\n",
    "\n",
    "## Granger Causality Test\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "def granger_causality_tests(df, target_col, predictor_cols, maxlag=12):\n",
    "    rows = []\n",
    "    for pred in predictor_cols:\n",
    "        test_result = grangercausalitytests(df[[target_col, pred]], maxlag=maxlag, verbose=False)\n",
    "        # Extract all p-values for the predictor across lags\n",
    "        lag_pvals = {lag: test_result[lag][0]['ssr_ftest'][1] for lag in test_result}\n",
    "        \n",
    "        # Find lags with significant p-values\n",
    "        significant_lags = [lag for lag, pval in lag_pvals.items() if pval <= 0.05]\n",
    "        if significant_lags:\n",
    "            # Find minimum p-value and corresponding lag\n",
    "            min_lag = min(significant_lags, key=lambda lag: lag_pvals[lag])\n",
    "            min_pval = lag_pvals[min_lag]\n",
    "            rows.append({\n",
    "                'predictor': pred,\n",
    "                'min_pvalue_lag': min_lag,\n",
    "                'min_pvalue': min_pval,\n",
    "                'significant_lags': significant_lags\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(rows).sort_values('min_pvalue').reset_index(drop=True)\n",
    "    return results_df\n",
    "\n",
    "## Correlations With Lags\n",
    "def lagged_correlations(df, target_col, predictor_cols, maxlag=12):\n",
    "    results = []\n",
    "    target = df[target_col]\n",
    "    \n",
    "    for pred in predictor_cols:\n",
    "        lag_corrs = {lag: target.corr(df[pred].shift(lag)) for lag in range(maxlag + 1)}\n",
    "        sig_lags = [lag for lag, c in lag_corrs.items() if pd.notna(c)]\n",
    "        \n",
    "        if sig_lags:\n",
    "            best_lag = max(sig_lags, key=lambda lag: abs(lag_corrs[lag]))\n",
    "            results.append({\n",
    "                'predictor': pred,\n",
    "                'max_corr_lag': best_lag,\n",
    "                'max_abs_corr': abs(lag_corrs[best_lag]),\n",
    "                'significant_lags': sig_lags\n",
    "            })\n",
    "        else:\n",
    "            results.append({'predictor': pred, 'max_corr_lag': None, 'max_abs_corr': None, 'significant_lags': []})\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values(by='max_abs_corr', key=lambda x: x.abs(), ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "def plot_multi_plots(df, date_col, target_col, predictor_cols,  ncols=3, figsize_per_plot=(6, 3)):\n",
    "    nplots = len(predictor_cols)\n",
    "    nrows = (nplots + ncols - 1) // ncols\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(figsize_per_plot[0]*ncols, figsize_per_plot[1]*nrows), sharex=True)\n",
    "    axes = axes.flatten() if nplots > 1 else [axes]\n",
    "\n",
    "    for i, pred in enumerate(predictor_cols):\n",
    "        ax1 = axes[i]\n",
    "        ax2 = ax1.twinx()  # create a second y-axis\n",
    "\n",
    "        # Plot target on primary y-axis\n",
    "        ax1.plot(df[date_col], df[target_col], color='blue', label=target_col)\n",
    "        ax1.set_ylabel(target_col, color='blue')\n",
    "        ax1.tick_params(axis='y', colors='blue')\n",
    "\n",
    "        # Plot predictor on secondary y-axis\n",
    "        ax2.plot(df[date_col], df[pred], color='red', label=pred)\n",
    "        ax2.set_ylabel(pred, color='red')\n",
    "        ax2.tick_params(axis='y', colors='red')\n",
    "\n",
    "        ax1.set_title(f\"{target_col} vs {pred}\")\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # Combine legends from both axes\n",
    "        lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "        lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper left')\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0469aa53-96a0-471b-a3a0-67cc999a657b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# decompose target and variables into STL components (Seasonal, Trend and Residual)\n",
    "target_col = 'household_deposits'\n",
    "date_col = 'date'\n",
    "predictor_cols = [col for col in df.columns if col not in ['date', 'household_deposits']]\n",
    "tdf, sdf, rdf = decompose_stl(df, date_col, target_col, predictor_cols, period=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b14234ba-d704-4c15-af18-6f0be3533538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_col = 'household_deposits'\n",
    "date_col = 'date'\n",
    "predictor_cols = [col for col in df.columns if col not in ['date', 'household_deposits']]\n",
    "\n",
    "# test on stl decomposition - trend\n",
    "trend_granger = granger_causality_tests(tdf, target_col, predictor_cols)\n",
    "trend_lagcorr = lagged_correlations(tdf, target_col, predictor_cols)\n",
    "\n",
    "# test on stl decomposition - seasonality\n",
    "seasonal_granger = granger_causality_tests(sdf, target_col, predictor_cols)\n",
    "seasonal_lagcorr = lagged_correlations(sdf, target_col, predictor_cols)\n",
    "\n",
    "# test on stl decomposition - residual\n",
    "residual_granger = granger_causality_tests(rdf, target_col, predictor_cols)\n",
    "residual_lagcorr = lagged_correlations(rdf, target_col, predictor_cols)\n",
    "\n",
    "## Plot lagged correlations\n",
    "plot_target_correlation(trend_lagcorr.set_index('predictor'), 'max_abs_corr', 'trend')\n",
    "plot_target_correlation(seasonal_lagcorr.set_index('predictor'), 'max_abs_corr', 'seasonal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f64d88b-3e5d-4cc1-baf6-99fdc67ce342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prepare_component(lagcorr_df, granger_df, component):\n",
    "    df = pd.merge(\n",
    "        lagcorr_df, granger_df,\n",
    "        on='predictor', how='outer',\n",
    "        suffixes=('_corr', '_granger')\n",
    "    )\n",
    "    df.columns = ['predictor'] + [\n",
    "        f\"{component}_{col}\" if col != 'predictor' else col\n",
    "        for col in df.columns[1:]\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "# Prepare each component\n",
    "trend_df = prepare_component(trend_lagcorr, trend_granger, 'trend')\n",
    "seasonal_df = prepare_component(seasonal_lagcorr, seasonal_granger, 'seasonal')\n",
    "residual_df = prepare_component(residual_lagcorr, residual_granger, 'residual')\n",
    "\n",
    "# Merge all components on 'predictor'\n",
    "pivoted_result = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on='predictor', how='outer'),\n",
    "    [trend_df, seasonal_df, residual_df]\n",
    ").sort_values('predictor').reset_index(drop=True)\n",
    "\n",
    "top_predictors = pivoted_result[['predictor','trend_max_abs_corr','trend_min_pvalue','trend_max_corr_lag','trend_min_pvalue_lag','seasonal_max_abs_corr','seasonal_min_pvalue','seasonal_max_corr_lag','seasonal_min_pvalue_lag']].sort_values(by='trend_min_pvalue', ascending=True).dropna()\n",
    "\n",
    "# Reduction of features based on the criteria that the features have to have a correlation with target > 0.9 and a significant p-value (<=0.05) from the granger test\n",
    "signifnicant_corr_granger = top_predictors[(top_predictors.trend_max_abs_corr>0.9) &(top_predictors.seasonal_min_pvalue <=0.05)]\n",
    "signifnicant_corr_granger.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f8a639c-fadb-4570-b0ce-55db2b5f2ed3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The criteria above to select features with correlation > 0.9 with the target as well as a significant p-value from the granger test (<=0.05) eliminates almost half of the features where only 26 remain as plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "056024a7-95c5-40de-9473-6ca9a80cef59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot of all trend cols\n",
    "plot_multi_plots(tdf.reset_index(), 'date', target_col, signifnicant_corr_granger.predictor, ncols=3, figsize_per_plot=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bccbbbd-65ea-478d-8aeb-4fb00caec777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Manual Filtering of Irregular Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71b7cb3c-bd65-4e90-9573-6a0969d9f7b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Through visual analysis, the features `card_apparel`, `card_services`, `card_hospitality`, `production_based_gdp`, `card_durables`, `total_card_transactions`, `card_motor_vehicles_excl_fuel`, `term_deposit_rate_google`, `card_non_retail_excl_services`, `hpi` were further removed due to irregular behaviour during the COVID-19 period around 20-21. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46909ed-38f8-47ea-9b52-6eb72b3dfe2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "excluded_cols = ['card_apparel','card_services','card_hospitality','production_based_gdp','card_durables','total_card_transactions','card_motor_vehicles_excl_fuel','term_deposit_rate_google','card_non_retail_excl_services','hpi']\n",
    "signifnicant_corr_granger = signifnicant_corr_granger[~signifnicant_corr_granger.predictor.isin(excluded_cols)]\n",
    "signifnicant_corr_granger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3c66f25-c761-4732-a06a-36bdc732e542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The remaining 16 significant predictors are listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e02ebfc-1f6f-49eb-a40b-3a3380f98702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list(signifnicant_corr_granger.predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8038a042-bfc1-4627-9a0b-d1d874e78274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Further Filtering Based on Availability of Future Forecasts\n",
    "For quarterly data, since they are interpolated, a future or forecasted value is required to make predictions for future horizons. As such, an additional round of feature reduction was conducted on the 16 predictors listed above, retaining only those features for which forecasted values were available. See datalog in the data folder for further details. Remaining predictors after filtering are as below:\n",
    "\n",
    "`card_consumables`, `card_credit`, `household_loans`, `google_home_loan_rate_search`, `unemployment_rate`, `employed_labour_force`, `labour_force_participation_rate`\n",
    ", `labour_cost_index`, `cpi`, `cpi_housing_household`, `production_based_gdp`\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_EDA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
